<!DOCTYPE html>
<html>
	<head>
		<script src="/bjc-r/llab/loader.js"></script>
		<title>Algorithmic Bias and the Moral Machine</title>
	</head>
	<body>
		<h2>The MIT Moral Machine</h2>
		<p>
			With the development of self-driving cars come various challenges. Many of those
			challenges are technical, but some of them are also ethical. <a href="http://moralmachine.mit.edu/">The Moral Machine</a>
			is a project from the MIT Media Lab that aims to help users explore various 
			ethical dillemas that self-driving cars may face and gather perspective
			on how people think they should behave. 
		</p>
		<p>
			The Moral Machine presents users with various scenarios that a self-driving car
			may find itself in, and allows the users to choose how they believe the car should
			behave. In each of the situations, the car finds itself in a situation where it 
			<strong>must</strong> choose between saving the passengers in the car or the pedestrians
			crossing the street. 
		</p>
		<img src="/bjc-r/img/moral-machine/scenario_example.png" alt="example scenario"/>
		<p>
			However, this problem isn't as straightforward as it seems. For example, you may want to consider the number of passengers vs the number of pedestrians, whether the pedestrians were crossing legally, and perhaps even the characteristics of each individual (profession, age, etc.). 
		</p>
		<p>
			In today's lab, you will create a decision algorithm in Python that chooses who should be saved in a possible Moral Machine scenario.
		</p>
	</body>
</html>